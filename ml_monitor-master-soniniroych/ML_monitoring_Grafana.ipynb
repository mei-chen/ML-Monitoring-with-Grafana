{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we will apply data generator to monitor test outcome on Grafana dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data set is Boston Housing Prices classification data set https://www.kaggle.com/vikrishnan/boston-house-prices\n",
    "## Step 1: Pre-process the data (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features per sample= 13\n"
     ]
    }
   ],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "train_features = (train_features - train_mean) / train_std\n",
    "print(\"Number of features per sample=\", np.shape(train_features)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for test features\n",
    "test_mean = np.mean(test_features, axis=0)\n",
    "test_std = np.std(test_features, axis=0)\n",
    "test_features = (test_features - test_mean) / test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a simple Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(100, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(), \n",
    "                  loss='mse',\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 571.1794 - mae: 22.0055 - mse: 571.1794 - val_loss: 470.9334 - val_mae: 20.7350 - val_mse: 470.9334\n",
      "Epoch 2/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 544.7949 - mae: 21.4656 - mse: 544.7949 - val_loss: 447.0529 - val_mae: 20.1929 - val_mse: 447.0529\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 519.2650 - mae: 20.9279 - mse: 519.2650 - val_loss: 422.1715 - val_mae: 19.6118 - val_mse: 422.1715\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 490.5690 - mae: 20.3091 - mse: 490.5690 - val_loss: 396.0872 - val_mae: 18.9712 - val_mse: 396.0872\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 461.0384 - mae: 19.6456 - mse: 461.0384 - val_loss: 366.7555 - val_mae: 18.2316 - val_mse: 366.7555\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 427.4960 - mae: 18.8704 - mse: 427.4960 - val_loss: 335.5804 - val_mae: 17.3980 - val_mse: 335.5804\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 391.8694 - mae: 17.9985 - mse: 391.8694 - val_loss: 302.6610 - val_mae: 16.4611 - val_mse: 302.6610\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 355.5010 - mae: 17.0389 - mse: 355.5010 - val_loss: 268.7338 - val_mae: 15.4138 - val_mse: 268.7338\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 317.9943 - mae: 15.9667 - mse: 317.9943 - val_loss: 234.4234 - val_mae: 14.2637 - val_mse: 234.4234\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 279.6441 - mae: 14.8226 - mse: 279.6441 - val_loss: 202.5627 - val_mae: 13.1148 - val_mse: 202.5627\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 244.5862 - mae: 13.6307 - mse: 244.5862 - val_loss: 172.4785 - val_mae: 11.9019 - val_mse: 172.4785\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 212.5145 - mae: 12.4305 - mse: 212.5145 - val_loss: 145.7748 - val_mae: 10.6671 - val_mse: 145.7748\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 182.8331 - mae: 11.3058 - mse: 182.8331 - val_loss: 123.5916 - val_mae: 9.5691 - val_mse: 123.5916\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 158.6467 - mae: 10.3421 - mse: 158.6467 - val_loss: 105.0893 - val_mae: 8.6932 - val_mse: 105.0893\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 137.6143 - mae: 9.4872 - mse: 137.6143 - val_loss: 90.5779 - val_mae: 8.0977 - val_mse: 90.5779\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.1231 - mae: 8.7568 - mse: 121.1231 - val_loss: 78.7370 - val_mae: 7.5421 - val_mse: 78.7370\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 106.5446 - mae: 8.1299 - mse: 106.5446 - val_loss: 69.2457 - val_mae: 7.0823 - val_mse: 69.2457\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 94.0654 - mae: 7.5634 - mse: 94.0654 - val_loss: 61.6620 - val_mae: 6.6644 - val_mse: 61.6620\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.9545 - mae: 7.1018 - mse: 83.9545 - val_loss: 54.9730 - val_mae: 6.2489 - val_mse: 54.9730\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.6092 - mae: 6.6796 - mse: 74.6092 - val_loss: 49.3048 - val_mae: 5.9188 - val_mse: 49.3048\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 66.9294 - mae: 6.2830 - mse: 66.9294 - val_loss: 44.3150 - val_mae: 5.5720 - val_mse: 44.3150\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 59.9931 - mae: 5.9170 - mse: 59.9931 - val_loss: 40.2304 - val_mae: 5.2688 - val_mse: 40.2304\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 54.2655 - mae: 5.5908 - mse: 54.2655 - val_loss: 36.7955 - val_mae: 5.0305 - val_mse: 36.7955\n",
      "Epoch 24/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 49.1352 - mae: 5.2761 - mse: 49.1352 - val_loss: 33.7634 - val_mae: 4.7904 - val_mse: 33.7634\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 44.7648 - mae: 4.9954 - mse: 44.7648 - val_loss: 31.5190 - val_mae: 4.5847 - val_mse: 31.5190\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 41.1891 - mae: 4.7565 - mse: 41.1891 - val_loss: 29.3540 - val_mae: 4.3935 - val_mse: 29.3540\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 38.1246 - mae: 4.5365 - mse: 38.1246 - val_loss: 27.6988 - val_mae: 4.2716 - val_mse: 27.6988\n",
      "Epoch 28/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 35.4747 - mae: 4.3385 - mse: 35.4747 - val_loss: 26.4668 - val_mae: 4.1933 - val_mse: 26.4668\n",
      "Epoch 29/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 33.4279 - mae: 4.1574 - mse: 33.4279 - val_loss: 25.6379 - val_mae: 4.1218 - val_mse: 25.6379\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 31.7002 - mae: 4.0180 - mse: 31.7002 - val_loss: 24.8879 - val_mae: 4.0454 - val_mse: 24.8879\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 30.2482 - mae: 3.8985 - mse: 30.2482 - val_loss: 24.2986 - val_mae: 3.9717 - val_mse: 24.2986\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 29.2834 - mae: 3.8214 - mse: 29.2834 - val_loss: 23.8145 - val_mae: 3.9044 - val_mse: 23.8145\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 28.3001 - mae: 3.7397 - mse: 28.3001 - val_loss: 23.4743 - val_mae: 3.8585 - val_mse: 23.4743\n",
      "Epoch 34/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 27.5467 - mae: 3.6826 - mse: 27.5467 - val_loss: 23.2280 - val_mae: 3.8127 - val_mse: 23.2280\n",
      "Epoch 35/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 26.8786 - mae: 3.6377 - mse: 26.8786 - val_loss: 23.0258 - val_mae: 3.7726 - val_mse: 23.0258\n",
      "Epoch 36/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 26.2427 - mae: 3.5960 - mse: 26.2427 - val_loss: 22.6519 - val_mae: 3.7186 - val_mse: 22.6519\n",
      "Epoch 37/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 25.7157 - mae: 3.5644 - mse: 25.7157 - val_loss: 22.3573 - val_mae: 3.6734 - val_mse: 22.3573\n",
      "Epoch 38/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 25.1818 - mae: 3.5416 - mse: 25.1818 - val_loss: 22.3845 - val_mae: 3.6703 - val_mse: 22.3845\n",
      "Epoch 39/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 24.6902 - mae: 3.5205 - mse: 24.6902 - val_loss: 22.1953 - val_mae: 3.6492 - val_mse: 22.1953\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 24.2476 - mae: 3.4916 - mse: 24.2476 - val_loss: 22.1264 - val_mae: 3.6362 - val_mse: 22.1264\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 23.8298 - mae: 3.4657 - mse: 23.8298 - val_loss: 21.9849 - val_mae: 3.6299 - val_mse: 21.9849\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 23.4395 - mae: 3.4400 - mse: 23.4395 - val_loss: 21.7752 - val_mae: 3.6169 - val_mse: 21.7752\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 23.0713 - mae: 3.4169 - mse: 23.0713 - val_loss: 21.6011 - val_mae: 3.6000 - val_mse: 21.6011\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 22.7298 - mae: 3.3972 - mse: 22.7298 - val_loss: 21.5300 - val_mae: 3.5943 - val_mse: 21.5300\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 22.3769 - mae: 3.3709 - mse: 22.3769 - val_loss: 21.2326 - val_mae: 3.5665 - val_mse: 21.2326\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 22.0512 - mae: 3.3482 - mse: 22.0512 - val_loss: 21.1980 - val_mae: 3.5650 - val_mse: 21.1980\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 21.7310 - mae: 3.3265 - mse: 21.7310 - val_loss: 20.9816 - val_mae: 3.5439 - val_mse: 20.9816\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 21.4037 - mae: 3.3079 - mse: 21.4037 - val_loss: 20.8095 - val_mae: 3.5340 - val_mse: 20.8095\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 21.1182 - mae: 3.2917 - mse: 21.1182 - val_loss: 20.6236 - val_mae: 3.5200 - val_mse: 20.6236\n",
      "Epoch 50/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 20.7996 - mae: 3.2654 - mse: 20.7996 - val_loss: 20.6221 - val_mae: 3.5210 - val_mse: 20.6221\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 20.5428 - mae: 3.2466 - mse: 20.5428 - val_loss: 20.5240 - val_mae: 3.5150 - val_mse: 20.5240\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 20.2127 - mae: 3.2177 - mse: 20.2127 - val_loss: 20.2547 - val_mae: 3.4855 - val_mse: 20.2547\n",
      "Epoch 53/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 19.9286 - mae: 3.1916 - mse: 19.9286 - val_loss: 19.7926 - val_mae: 3.4421 - val_mse: 19.7926\n",
      "Epoch 54/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 19.6781 - mae: 3.1670 - mse: 19.6781 - val_loss: 19.5883 - val_mae: 3.4253 - val_mse: 19.5883\n",
      "Epoch 55/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 19.4162 - mae: 3.1419 - mse: 19.4162 - val_loss: 19.2313 - val_mae: 3.3910 - val_mse: 19.2313\n",
      "Epoch 56/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 19.1585 - mae: 3.1214 - mse: 19.1585 - val_loss: 19.2335 - val_mae: 3.3963 - val_mse: 19.2335\n",
      "Epoch 57/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 18.9135 - mae: 3.1027 - mse: 18.9135 - val_loss: 19.0139 - val_mae: 3.3798 - val_mse: 19.0139\n",
      "Epoch 58/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 18.6284 - mae: 3.0821 - mse: 18.6284 - val_loss: 18.8023 - val_mae: 3.3654 - val_mse: 18.8023\n",
      "Epoch 59/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 18.4260 - mae: 3.0681 - mse: 18.4260 - val_loss: 18.4723 - val_mae: 3.3281 - val_mse: 18.4723\n",
      "Epoch 60/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 18.1485 - mae: 3.0486 - mse: 18.1485 - val_loss: 18.4769 - val_mae: 3.3382 - val_mse: 18.4769\n",
      "Epoch 61/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 17.8808 - mae: 3.0256 - mse: 17.8808 - val_loss: 18.1904 - val_mae: 3.3099 - val_mse: 18.1904\n",
      "Epoch 62/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 17.6412 - mae: 3.0069 - mse: 17.6412 - val_loss: 18.0019 - val_mae: 3.2914 - val_mse: 18.0019\n",
      "Epoch 63/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 17.4237 - mae: 2.9839 - mse: 17.4237 - val_loss: 17.6713 - val_mae: 3.2678 - val_mse: 17.6713\n",
      "Epoch 64/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 17.1920 - mae: 2.9622 - mse: 17.1920 - val_loss: 17.3653 - val_mae: 3.2411 - val_mse: 17.3653\n",
      "Epoch 65/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 16.9604 - mae: 2.9410 - mse: 16.9604 - val_loss: 17.2383 - val_mae: 3.2298 - val_mse: 17.2383\n",
      "Epoch 66/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 16.7596 - mae: 2.9256 - mse: 16.7596 - val_loss: 17.2059 - val_mae: 3.2386 - val_mse: 17.2059\n",
      "Epoch 67/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 16.5437 - mae: 2.9018 - mse: 16.5437 - val_loss: 16.9729 - val_mae: 3.2225 - val_mse: 16.9729\n",
      "Epoch 68/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 16.3103 - mae: 2.8791 - mse: 16.3103 - val_loss: 16.6333 - val_mae: 3.1840 - val_mse: 16.6333\n",
      "Epoch 69/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 16.1165 - mae: 2.8574 - mse: 16.1165 - val_loss: 16.3639 - val_mae: 3.1604 - val_mse: 16.3639\n",
      "Epoch 70/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 15.9312 - mae: 2.8403 - mse: 15.9312 - val_loss: 16.2785 - val_mae: 3.1484 - val_mse: 16.2785\n",
      "Epoch 71/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 15.7160 - mae: 2.8217 - mse: 15.7160 - val_loss: 15.9317 - val_mae: 3.1115 - val_mse: 15.9317\n",
      "Epoch 72/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 15.5338 - mae: 2.8042 - mse: 15.5338 - val_loss: 15.7915 - val_mae: 3.1041 - val_mse: 15.7915\n",
      "Epoch 73/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 15.3519 - mae: 2.7811 - mse: 15.3519 - val_loss: 15.7263 - val_mae: 3.1156 - val_mse: 15.7263\n",
      "Epoch 74/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 15.1781 - mae: 2.7585 - mse: 15.1781 - val_loss: 15.6604 - val_mae: 3.1135 - val_mse: 15.6604\n",
      "Epoch 75/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 14.9942 - mae: 2.7415 - mse: 14.9942 - val_loss: 15.2346 - val_mae: 3.0597 - val_mse: 15.2346\n",
      "Epoch 76/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 14.8331 - mae: 2.7267 - mse: 14.8331 - val_loss: 14.9718 - val_mae: 3.0234 - val_mse: 14.9718\n",
      "Epoch 77/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 14.6584 - mae: 2.7108 - mse: 14.6584 - val_loss: 14.8381 - val_mae: 3.0203 - val_mse: 14.8381\n",
      "Epoch 78/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 14.4942 - mae: 2.6935 - mse: 14.4942 - val_loss: 14.7779 - val_mae: 3.0190 - val_mse: 14.7779\n",
      "Epoch 79/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 14.3391 - mae: 2.6732 - mse: 14.3391 - val_loss: 14.4752 - val_mae: 2.9892 - val_mse: 14.4752\n",
      "Epoch 80/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 14.1871 - mae: 2.6589 - mse: 14.1871 - val_loss: 14.2983 - val_mae: 2.9575 - val_mse: 14.2983\n",
      "Epoch 81/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 14.0182 - mae: 2.6472 - mse: 14.0182 - val_loss: 14.1548 - val_mae: 2.9538 - val_mse: 14.1548\n",
      "Epoch 82/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 13.8637 - mae: 2.6311 - mse: 13.8637 - val_loss: 14.0020 - val_mae: 2.9368 - val_mse: 14.0020\n",
      "Epoch 83/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 13.7226 - mae: 2.6177 - mse: 13.7226 - val_loss: 13.8322 - val_mae: 2.9204 - val_mse: 13.8322\n",
      "Epoch 84/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 13.6090 - mae: 2.6023 - mse: 13.6090 - val_loss: 13.8558 - val_mae: 2.9550 - val_mse: 13.8558\n",
      "Epoch 85/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 13.4821 - mae: 2.5811 - mse: 13.4821 - val_loss: 13.4596 - val_mae: 2.9210 - val_mse: 13.4596\n",
      "Epoch 86/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 13.3101 - mae: 2.5712 - mse: 13.3101 - val_loss: 13.2712 - val_mae: 2.8694 - val_mse: 13.2712\n",
      "Epoch 87/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 13.1816 - mae: 2.5717 - mse: 13.1816 - val_loss: 13.1216 - val_mae: 2.8432 - val_mse: 13.1216\n",
      "Epoch 88/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 13.0453 - mae: 2.5639 - mse: 13.0453 - val_loss: 13.0429 - val_mae: 2.8332 - val_mse: 13.0429\n",
      "Epoch 89/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 12.9014 - mae: 2.5468 - mse: 12.9014 - val_loss: 13.1058 - val_mae: 2.8651 - val_mse: 13.1058\n",
      "Epoch 90/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.7997 - mae: 2.5373 - mse: 12.7997 - val_loss: 12.8714 - val_mae: 2.8430 - val_mse: 12.8714\n",
      "Epoch 91/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.6625 - mae: 2.5206 - mse: 12.6625 - val_loss: 12.6949 - val_mae: 2.8449 - val_mse: 12.6949\n",
      "Epoch 92/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.5712 - mae: 2.5082 - mse: 12.5712 - val_loss: 12.6728 - val_mae: 2.8491 - val_mse: 12.6728\n",
      "Epoch 93/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.4424 - mae: 2.4941 - mse: 12.4424 - val_loss: 12.3255 - val_mae: 2.7986 - val_mse: 12.3255\n",
      "Epoch 94/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.3071 - mae: 2.4736 - mse: 12.3071 - val_loss: 12.0632 - val_mae: 2.7938 - val_mse: 12.0632\n",
      "Epoch 95/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 12.2133 - mae: 2.4629 - mse: 12.2133 - val_loss: 11.9616 - val_mae: 2.7715 - val_mse: 11.9616\n",
      "Epoch 96/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 12.1328 - mae: 2.4568 - mse: 12.1328 - val_loss: 11.6608 - val_mae: 2.7327 - val_mse: 11.6608\n",
      "Epoch 97/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 12.0634 - mae: 2.4442 - mse: 12.0634 - val_loss: 11.7073 - val_mae: 2.7606 - val_mse: 11.7073\n",
      "Epoch 98/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 11.9973 - mae: 2.4464 - mse: 11.9973 - val_loss: 11.5723 - val_mae: 2.6911 - val_mse: 11.5723\n",
      "Epoch 99/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 11.8154 - mae: 2.4362 - mse: 11.8154 - val_loss: 11.4425 - val_mae: 2.6859 - val_mse: 11.4425\n",
      "Epoch 100/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.7298 - mae: 2.4159 - mse: 11.7298 - val_loss: 11.4354 - val_mae: 2.7368 - val_mse: 11.4354\n",
      "Epoch 101/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.6115 - mae: 2.3991 - mse: 11.6115 - val_loss: 11.3263 - val_mae: 2.7128 - val_mse: 11.3263\n",
      "Epoch 102/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.5305 - mae: 2.3984 - mse: 11.5305 - val_loss: 11.1132 - val_mae: 2.6604 - val_mse: 11.1132\n",
      "Epoch 103/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.4437 - mae: 2.3854 - mse: 11.4437 - val_loss: 11.0945 - val_mae: 2.6908 - val_mse: 11.0945\n",
      "Epoch 104/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.3309 - mae: 2.3694 - mse: 11.3309 - val_loss: 10.9015 - val_mae: 2.6610 - val_mse: 10.9015\n",
      "Epoch 105/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 11.2565 - mae: 2.3669 - mse: 11.2565 - val_loss: 10.8196 - val_mae: 2.6387 - val_mse: 10.8196\n",
      "Epoch 106/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 11.1969 - mae: 2.3555 - mse: 11.1969 - val_loss: 10.7662 - val_mae: 2.6739 - val_mse: 10.7662\n",
      "Epoch 107/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.0897 - mae: 2.3412 - mse: 11.0897 - val_loss: 10.4612 - val_mae: 2.6024 - val_mse: 10.4612\n",
      "Epoch 108/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 11.0485 - mae: 2.3354 - mse: 11.0485 - val_loss: 10.4159 - val_mae: 2.6163 - val_mse: 10.4159\n",
      "Epoch 109/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.9386 - mae: 2.3293 - mse: 10.9386 - val_loss: 10.2203 - val_mae: 2.5646 - val_mse: 10.2203\n",
      "Epoch 110/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.8741 - mae: 2.3227 - mse: 10.8741 - val_loss: 10.1483 - val_mae: 2.5704 - val_mse: 10.1483\n",
      "Epoch 111/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.7990 - mae: 2.3148 - mse: 10.7990 - val_loss: 10.2834 - val_mae: 2.6005 - val_mse: 10.2834\n",
      "Epoch 112/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.7371 - mae: 2.3083 - mse: 10.7371 - val_loss: 10.3314 - val_mae: 2.6258 - val_mse: 10.3314\n",
      "Epoch 113/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.6696 - mae: 2.2962 - mse: 10.6696 - val_loss: 10.0866 - val_mae: 2.5835 - val_mse: 10.0866\n",
      "Epoch 114/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.5931 - mae: 2.2875 - mse: 10.5931 - val_loss: 10.1238 - val_mae: 2.5783 - val_mse: 10.1238\n",
      "Epoch 115/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 10.5347 - mae: 2.2832 - mse: 10.5347 - val_loss: 9.9537 - val_mae: 2.5506 - val_mse: 9.9537\n",
      "Epoch 116/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 10.4785 - mae: 2.2706 - mse: 10.4785 - val_loss: 9.7674 - val_mae: 2.5510 - val_mse: 9.7674\n",
      "Epoch 117/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.4462 - mae: 2.2662 - mse: 10.4462 - val_loss: 9.6468 - val_mae: 2.5067 - val_mse: 9.6468\n",
      "Epoch 118/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 10.3384 - mae: 2.2543 - mse: 10.3384 - val_loss: 9.4784 - val_mae: 2.5242 - val_mse: 9.4784\n",
      "Epoch 119/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.2999 - mae: 2.2491 - mse: 10.2999 - val_loss: 9.3696 - val_mae: 2.4908 - val_mse: 9.3696\n",
      "Epoch 120/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 10.2217 - mae: 2.2421 - mse: 10.2217 - val_loss: 9.3329 - val_mae: 2.4997 - val_mse: 9.3329\n",
      "Epoch 121/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 10.1584 - mae: 2.2365 - mse: 10.1584 - val_loss: 9.1930 - val_mae: 2.4855 - val_mse: 9.1930\n",
      "Epoch 122/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.1351 - mae: 2.2332 - mse: 10.1351 - val_loss: 9.1441 - val_mae: 2.5135 - val_mse: 9.1441\n",
      "Epoch 123/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 10.0620 - mae: 2.2263 - mse: 10.0620 - val_loss: 9.0867 - val_mae: 2.4873 - val_mse: 9.0867\n",
      "Epoch 124/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.9991 - mae: 2.2163 - mse: 9.9991 - val_loss: 8.8866 - val_mae: 2.4585 - val_mse: 8.8866\n",
      "Epoch 125/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.9663 - mae: 2.2082 - mse: 9.9663 - val_loss: 8.7387 - val_mae: 2.4191 - val_mse: 8.7387\n",
      "Epoch 126/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.9065 - mae: 2.2078 - mse: 9.9065 - val_loss: 8.8557 - val_mae: 2.4141 - val_mse: 8.8557\n",
      "Epoch 127/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.8770 - mae: 2.2034 - mse: 9.8770 - val_loss: 8.7951 - val_mae: 2.4472 - val_mse: 8.7951\n",
      "Epoch 128/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.8261 - mae: 2.1982 - mse: 9.8261 - val_loss: 8.6653 - val_mae: 2.4328 - val_mse: 8.6653\n",
      "Epoch 129/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.7710 - mae: 2.1941 - mse: 9.7710 - val_loss: 8.5916 - val_mae: 2.4217 - val_mse: 8.5916\n",
      "Epoch 130/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.7426 - mae: 2.1882 - mse: 9.7426 - val_loss: 8.5594 - val_mae: 2.4234 - val_mse: 8.5594\n",
      "Epoch 131/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.6941 - mae: 2.1834 - mse: 9.6941 - val_loss: 8.4516 - val_mae: 2.3994 - val_mse: 8.4516\n",
      "Epoch 132/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.6651 - mae: 2.1791 - mse: 9.6651 - val_loss: 8.3955 - val_mae: 2.4074 - val_mse: 8.3955\n",
      "Epoch 133/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.6316 - mae: 2.1751 - mse: 9.6316 - val_loss: 8.3673 - val_mae: 2.3981 - val_mse: 8.3673\n",
      "Epoch 134/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.5788 - mae: 2.1657 - mse: 9.5788 - val_loss: 8.4240 - val_mae: 2.4174 - val_mse: 8.4240\n",
      "Epoch 135/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.5830 - mae: 2.1721 - mse: 9.5830 - val_loss: 8.5627 - val_mae: 2.4457 - val_mse: 8.5627\n",
      "Epoch 136/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.5077 - mae: 2.1612 - mse: 9.5077 - val_loss: 8.3554 - val_mae: 2.3917 - val_mse: 8.3554\n",
      "Epoch 137/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.4999 - mae: 2.1606 - mse: 9.4999 - val_loss: 8.3512 - val_mae: 2.3740 - val_mse: 8.3512\n",
      "Epoch 138/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.5221 - mae: 2.1763 - mse: 9.5221 - val_loss: 8.3441 - val_mae: 2.3605 - val_mse: 8.3441\n",
      "Epoch 139/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.4373 - mae: 2.1590 - mse: 9.4373 - val_loss: 8.3297 - val_mae: 2.4044 - val_mse: 8.3297\n",
      "Epoch 140/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.3837 - mae: 2.1508 - mse: 9.3837 - val_loss: 8.1842 - val_mae: 2.3754 - val_mse: 8.1842\n",
      "Epoch 141/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.3810 - mae: 2.1394 - mse: 9.3810 - val_loss: 8.0309 - val_mae: 2.3706 - val_mse: 8.0309\n",
      "Epoch 142/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.3498 - mae: 2.1410 - mse: 9.3498 - val_loss: 8.1374 - val_mae: 2.4028 - val_mse: 8.1374\n",
      "Epoch 143/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.3864 - mae: 2.1514 - mse: 9.3864 - val_loss: 8.0727 - val_mae: 2.3521 - val_mse: 8.0727\n",
      "Epoch 144/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.2759 - mae: 2.1445 - mse: 9.2759 - val_loss: 7.9524 - val_mae: 2.3698 - val_mse: 7.9524\n",
      "Epoch 145/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.2295 - mae: 2.1276 - mse: 9.2295 - val_loss: 7.9473 - val_mae: 2.3736 - val_mse: 7.9473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.2303 - mae: 2.1252 - mse: 9.2303 - val_loss: 7.9527 - val_mae: 2.3760 - val_mse: 7.9527\n",
      "Epoch 147/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.1784 - mae: 2.1226 - mse: 9.1784 - val_loss: 7.9733 - val_mae: 2.3746 - val_mse: 7.9733\n",
      "Epoch 148/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.1327 - mae: 2.1168 - mse: 9.1327 - val_loss: 7.7998 - val_mae: 2.3385 - val_mse: 7.7998\n",
      "Epoch 149/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.1194 - mae: 2.1137 - mse: 9.1194 - val_loss: 7.7848 - val_mae: 2.3550 - val_mse: 7.7848\n",
      "Epoch 150/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.0985 - mae: 2.1057 - mse: 9.0985 - val_loss: 7.5984 - val_mae: 2.3151 - val_mse: 7.5984\n",
      "Epoch 151/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.0752 - mae: 2.1062 - mse: 9.0752 - val_loss: 7.7141 - val_mae: 2.3487 - val_mse: 7.7141\n",
      "Epoch 152/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.0784 - mae: 2.1059 - mse: 9.0784 - val_loss: 7.6116 - val_mae: 2.3233 - val_mse: 7.6116\n",
      "Epoch 153/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 9.0223 - mae: 2.0923 - mse: 9.0223 - val_loss: 7.7515 - val_mae: 2.3737 - val_mse: 7.7515\n",
      "Epoch 154/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.0068 - mae: 2.0927 - mse: 9.0068 - val_loss: 7.6521 - val_mae: 2.3380 - val_mse: 7.6521\n",
      "Epoch 155/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 9.0073 - mae: 2.0907 - mse: 9.0073 - val_loss: 7.5714 - val_mae: 2.3408 - val_mse: 7.5714\n",
      "Epoch 156/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.9444 - mae: 2.0827 - mse: 8.9444 - val_loss: 7.5417 - val_mae: 2.3266 - val_mse: 7.5417\n",
      "Epoch 157/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.9234 - mae: 2.0843 - mse: 8.9234 - val_loss: 7.5637 - val_mae: 2.3236 - val_mse: 7.5637\n",
      "Epoch 158/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.9226 - mae: 2.0871 - mse: 8.9226 - val_loss: 7.7299 - val_mae: 2.3802 - val_mse: 7.7299\n",
      "Epoch 159/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.8860 - mae: 2.0806 - mse: 8.8860 - val_loss: 7.6070 - val_mae: 2.3448 - val_mse: 7.6070\n",
      "Epoch 160/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.9197 - mae: 2.0774 - mse: 8.9197 - val_loss: 7.4486 - val_mae: 2.2887 - val_mse: 7.4486\n",
      "Epoch 161/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.8310 - mae: 2.0704 - mse: 8.8310 - val_loss: 7.4976 - val_mae: 2.3279 - val_mse: 7.4976\n",
      "Epoch 162/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.8228 - mae: 2.0675 - mse: 8.8228 - val_loss: 7.4917 - val_mae: 2.3490 - val_mse: 7.4917\n",
      "Epoch 163/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.7990 - mae: 2.0625 - mse: 8.7990 - val_loss: 7.3820 - val_mae: 2.3225 - val_mse: 7.3820\n",
      "Epoch 164/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.7719 - mae: 2.0630 - mse: 8.7719 - val_loss: 7.3222 - val_mae: 2.3062 - val_mse: 7.3222\n",
      "Epoch 165/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.7891 - mae: 2.0679 - mse: 8.7891 - val_loss: 7.2395 - val_mae: 2.2580 - val_mse: 7.2395\n",
      "Epoch 166/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.7378 - mae: 2.0552 - mse: 8.7378 - val_loss: 7.4012 - val_mae: 2.3363 - val_mse: 7.4012\n",
      "Epoch 167/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.7308 - mae: 2.0577 - mse: 8.7308 - val_loss: 7.2909 - val_mae: 2.2940 - val_mse: 7.2909\n",
      "Epoch 168/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.6790 - mae: 2.0491 - mse: 8.6790 - val_loss: 7.2692 - val_mae: 2.2841 - val_mse: 7.2692\n",
      "Epoch 169/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.6564 - mae: 2.0485 - mse: 8.6564 - val_loss: 7.3780 - val_mae: 2.3207 - val_mse: 7.3780\n",
      "Epoch 170/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.6519 - mae: 2.0463 - mse: 8.6519 - val_loss: 7.1464 - val_mae: 2.2376 - val_mse: 7.1464\n",
      "Epoch 171/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.6485 - mae: 2.0490 - mse: 8.6485 - val_loss: 7.1534 - val_mae: 2.2701 - val_mse: 7.1534\n",
      "Epoch 172/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.6059 - mae: 2.0463 - mse: 8.6059 - val_loss: 7.2387 - val_mae: 2.2706 - val_mse: 7.2387\n",
      "Epoch 173/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5853 - mae: 2.0418 - mse: 8.5853 - val_loss: 7.3444 - val_mae: 2.3061 - val_mse: 7.3444\n",
      "Epoch 174/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5912 - mae: 2.0385 - mse: 8.5912 - val_loss: 7.2968 - val_mae: 2.3107 - val_mse: 7.2968\n",
      "Epoch 175/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5877 - mae: 2.0393 - mse: 8.5877 - val_loss: 7.2062 - val_mae: 2.2676 - val_mse: 7.2062\n",
      "Epoch 176/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.5306 - mae: 2.0317 - mse: 8.5306 - val_loss: 7.1450 - val_mae: 2.2754 - val_mse: 7.1450\n",
      "Epoch 177/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5369 - mae: 2.0337 - mse: 8.5369 - val_loss: 7.0290 - val_mae: 2.2393 - val_mse: 7.0290\n",
      "Epoch 178/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5061 - mae: 2.0318 - mse: 8.5061 - val_loss: 7.1070 - val_mae: 2.2781 - val_mse: 7.1070\n",
      "Epoch 179/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.5376 - mae: 2.0323 - mse: 8.5376 - val_loss: 7.1038 - val_mae: 2.2954 - val_mse: 7.1038\n",
      "Epoch 180/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.4835 - mae: 2.0221 - mse: 8.4835 - val_loss: 6.9424 - val_mae: 2.2274 - val_mse: 6.9424\n",
      "Epoch 181/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.4865 - mae: 2.0183 - mse: 8.4865 - val_loss: 7.0110 - val_mae: 2.2680 - val_mse: 7.0110\n",
      "Epoch 182/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.4623 - mae: 2.0315 - mse: 8.4623 - val_loss: 7.0252 - val_mae: 2.1979 - val_mse: 7.0252\n",
      "Epoch 183/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.4737 - mae: 2.0459 - mse: 8.4737 - val_loss: 7.0462 - val_mae: 2.2172 - val_mse: 7.0462\n",
      "Epoch 184/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.4182 - mae: 2.0303 - mse: 8.4182 - val_loss: 7.1332 - val_mae: 2.2923 - val_mse: 7.1332\n",
      "Epoch 185/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.3502 - mae: 2.0235 - mse: 8.3502 - val_loss: 7.0502 - val_mae: 2.2706 - val_mse: 7.0502\n",
      "Epoch 186/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.3484 - mae: 2.0171 - mse: 8.3484 - val_loss: 6.9765 - val_mae: 2.2679 - val_mse: 6.9765\n",
      "Epoch 187/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.3129 - mae: 2.0144 - mse: 8.3129 - val_loss: 6.8815 - val_mae: 2.2425 - val_mse: 6.8815\n",
      "Epoch 188/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.2876 - mae: 2.0079 - mse: 8.2876 - val_loss: 6.9714 - val_mae: 2.2618 - val_mse: 6.9714\n",
      "Epoch 189/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.2751 - mae: 2.0084 - mse: 8.2751 - val_loss: 6.7878 - val_mae: 2.2112 - val_mse: 6.7878\n",
      "Epoch 190/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.2708 - mae: 2.0089 - mse: 8.2708 - val_loss: 6.8396 - val_mae: 2.2419 - val_mse: 6.8396\n",
      "Epoch 191/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.2617 - mae: 1.9951 - mse: 8.2617 - val_loss: 6.7904 - val_mae: 2.2355 - val_mse: 6.7904\n",
      "Epoch 192/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.2602 - mae: 1.9966 - mse: 8.2602 - val_loss: 6.8062 - val_mae: 2.2480 - val_mse: 6.8062\n",
      "Epoch 193/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.1958 - mae: 1.9923 - mse: 8.1958 - val_loss: 6.6511 - val_mae: 2.2015 - val_mse: 6.6511\n",
      "Epoch 194/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.1857 - mae: 1.9945 - mse: 8.1857 - val_loss: 6.8001 - val_mae: 2.2401 - val_mse: 6.8001\n",
      "Epoch 195/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 8.1916 - mae: 1.9903 - mse: 8.1916 - val_loss: 6.6941 - val_mae: 2.2031 - val_mse: 6.6941\n",
      "Epoch 196/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.1587 - mae: 1.9897 - mse: 8.1587 - val_loss: 6.6065 - val_mae: 2.1862 - val_mse: 6.6065\n",
      "Epoch 197/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.1843 - mae: 1.9972 - mse: 8.1843 - val_loss: 6.7952 - val_mae: 2.2368 - val_mse: 6.7952\n",
      "Epoch 198/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.1101 - mae: 1.9841 - mse: 8.1101 - val_loss: 6.7370 - val_mae: 2.2306 - val_mse: 6.7370\n",
      "Epoch 199/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.1588 - mae: 1.9875 - mse: 8.1588 - val_loss: 6.6041 - val_mae: 2.1724 - val_mse: 6.6041\n",
      "Epoch 200/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.1052 - mae: 1.9819 - mse: 8.1052 - val_loss: 6.6433 - val_mae: 2.2124 - val_mse: 6.6433\n",
      "Epoch 201/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.0828 - mae: 1.9778 - mse: 8.0828 - val_loss: 6.6487 - val_mae: 2.2575 - val_mse: 6.6487\n",
      "Epoch 202/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.1452 - mae: 1.9814 - mse: 8.1452 - val_loss: 6.5726 - val_mae: 2.2361 - val_mse: 6.5726\n",
      "Epoch 203/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.0586 - mae: 1.9727 - mse: 8.0586 - val_loss: 6.4370 - val_mae: 2.1692 - val_mse: 6.4370\n",
      "Epoch 204/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.0360 - mae: 1.9712 - mse: 8.0360 - val_loss: 6.4200 - val_mae: 2.1530 - val_mse: 6.4200\n",
      "Epoch 205/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.9975 - mae: 1.9680 - mse: 7.9975 - val_loss: 6.4088 - val_mae: 2.1640 - val_mse: 6.4088\n",
      "Epoch 206/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.0276 - mae: 1.9600 - mse: 8.0276 - val_loss: 6.4496 - val_mae: 2.2029 - val_mse: 6.4496\n",
      "Epoch 207/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 8.0159 - mae: 1.9539 - mse: 8.0159 - val_loss: 6.3846 - val_mae: 2.1944 - val_mse: 6.3846\n",
      "Epoch 208/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.9529 - mae: 1.9608 - mse: 7.9529 - val_loss: 6.2975 - val_mae: 2.1402 - val_mse: 6.2975\n",
      "Epoch 209/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.9290 - mae: 1.9624 - mse: 7.9290 - val_loss: 6.4292 - val_mae: 2.1890 - val_mse: 6.4292\n",
      "Epoch 210/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.9357 - mae: 1.9569 - mse: 7.9357 - val_loss: 6.3855 - val_mae: 2.1777 - val_mse: 6.3855\n",
      "Epoch 211/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.9170 - mae: 1.9571 - mse: 7.9170 - val_loss: 6.2529 - val_mae: 2.1277 - val_mse: 6.2529\n",
      "Epoch 212/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8919 - mae: 1.9565 - mse: 7.8919 - val_loss: 6.4092 - val_mae: 2.1757 - val_mse: 6.4092\n",
      "Epoch 213/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8725 - mae: 1.9511 - mse: 7.8725 - val_loss: 6.4057 - val_mae: 2.1795 - val_mse: 6.4057\n",
      "Epoch 214/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8776 - mae: 1.9540 - mse: 7.8776 - val_loss: 6.1887 - val_mae: 2.1144 - val_mse: 6.1887\n",
      "Epoch 215/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8512 - mae: 1.9499 - mse: 7.8512 - val_loss: 6.2524 - val_mae: 2.1588 - val_mse: 6.2524\n",
      "Epoch 216/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8261 - mae: 1.9439 - mse: 7.8261 - val_loss: 6.3729 - val_mae: 2.1935 - val_mse: 6.3729\n",
      "Epoch 217/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.7905 - mae: 1.9393 - mse: 7.7905 - val_loss: 6.1622 - val_mae: 2.1048 - val_mse: 6.1622\n",
      "Epoch 218/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.8352 - mae: 1.9504 - mse: 7.8352 - val_loss: 6.1426 - val_mae: 2.1010 - val_mse: 6.1426\n",
      "Epoch 219/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7870 - mae: 1.9376 - mse: 7.7870 - val_loss: 6.3508 - val_mae: 2.1813 - val_mse: 6.3508\n",
      "Epoch 220/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7921 - mae: 1.9394 - mse: 7.7921 - val_loss: 6.2633 - val_mae: 2.1488 - val_mse: 6.2633\n",
      "Epoch 221/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7926 - mae: 1.9377 - mse: 7.7926 - val_loss: 6.2030 - val_mae: 2.1309 - val_mse: 6.2030\n",
      "Epoch 222/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7147 - mae: 1.9288 - mse: 7.7147 - val_loss: 6.3016 - val_mae: 2.1609 - val_mse: 6.3016\n",
      "Epoch 223/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7501 - mae: 1.9262 - mse: 7.7501 - val_loss: 6.2377 - val_mae: 2.1488 - val_mse: 6.2377\n",
      "Epoch 224/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7041 - mae: 1.9276 - mse: 7.7041 - val_loss: 6.0517 - val_mae: 2.1040 - val_mse: 6.0517\n",
      "Epoch 225/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7215 - mae: 1.9275 - mse: 7.7215 - val_loss: 6.0704 - val_mae: 2.0994 - val_mse: 6.0704\n",
      "Epoch 226/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.6588 - mae: 1.9178 - mse: 7.6588 - val_loss: 6.2253 - val_mae: 2.1472 - val_mse: 6.2253\n",
      "Epoch 227/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6517 - mae: 1.9223 - mse: 7.6517 - val_loss: 6.1241 - val_mae: 2.1209 - val_mse: 6.1241\n",
      "Epoch 228/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6164 - mae: 1.9161 - mse: 7.6164 - val_loss: 6.2071 - val_mae: 2.1508 - val_mse: 6.2071\n",
      "Epoch 229/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6259 - mae: 1.9119 - mse: 7.6259 - val_loss: 6.2326 - val_mae: 2.1587 - val_mse: 6.2326\n",
      "Epoch 230/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6488 - mae: 1.9178 - mse: 7.6488 - val_loss: 6.2257 - val_mae: 2.1391 - val_mse: 6.2257\n",
      "Epoch 231/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6001 - mae: 1.9127 - mse: 7.6001 - val_loss: 6.1515 - val_mae: 2.1357 - val_mse: 6.1515\n",
      "Epoch 232/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5697 - mae: 1.9080 - mse: 7.5697 - val_loss: 6.0097 - val_mae: 2.0922 - val_mse: 6.0097\n",
      "Epoch 233/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5754 - mae: 1.9117 - mse: 7.5754 - val_loss: 6.0881 - val_mae: 2.1159 - val_mse: 6.0881\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5389 - mae: 1.9073 - mse: 7.5389 - val_loss: 5.9998 - val_mae: 2.0803 - val_mse: 5.9998\n",
      "Epoch 235/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5321 - mae: 1.8995 - mse: 7.5321 - val_loss: 6.0490 - val_mae: 2.1173 - val_mse: 6.0490\n",
      "Epoch 236/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.5279 - mae: 1.9021 - mse: 7.5279 - val_loss: 6.0136 - val_mae: 2.0951 - val_mse: 6.0136\n",
      "Epoch 237/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5074 - mae: 1.8950 - mse: 7.5074 - val_loss: 6.1783 - val_mae: 2.1575 - val_mse: 6.1783\n",
      "Epoch 238/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5057 - mae: 1.8995 - mse: 7.5057 - val_loss: 6.1237 - val_mae: 2.1466 - val_mse: 6.1237\n",
      "Epoch 239/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.4607 - mae: 1.8915 - mse: 7.4607 - val_loss: 5.9622 - val_mae: 2.0664 - val_mse: 5.9622\n",
      "Epoch 240/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.5033 - mae: 1.8997 - mse: 7.5033 - val_loss: 6.0568 - val_mae: 2.1141 - val_mse: 6.0568\n",
      "Epoch 241/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.4335 - mae: 1.8891 - mse: 7.4335 - val_loss: 6.1233 - val_mae: 2.1204 - val_mse: 6.1233\n",
      "Epoch 242/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.4168 - mae: 1.8822 - mse: 7.4168 - val_loss: 5.9826 - val_mae: 2.1008 - val_mse: 5.9826\n",
      "Epoch 243/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.4205 - mae: 1.8861 - mse: 7.4205 - val_loss: 5.8716 - val_mae: 2.0804 - val_mse: 5.8716\n",
      "Epoch 244/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 7.4058 - mae: 1.8805 - mse: 7.4058 - val_loss: 5.9457 - val_mae: 2.1130 - val_mse: 5.9457\n",
      "Epoch 245/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.3688 - mae: 1.8805 - mse: 7.3688 - val_loss: 5.8353 - val_mae: 2.0558 - val_mse: 5.8353\n",
      "Epoch 246/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.4543 - mae: 1.9095 - mse: 7.4543 - val_loss: 5.7722 - val_mae: 2.0168 - val_mse: 5.7722\n",
      "Epoch 247/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.3896 - mae: 1.8961 - mse: 7.3896 - val_loss: 6.0061 - val_mae: 2.1088 - val_mse: 6.0061\n",
      "Epoch 248/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.3580 - mae: 1.8895 - mse: 7.3580 - val_loss: 6.0605 - val_mae: 2.1366 - val_mse: 6.0605\n",
      "Epoch 249/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.3754 - mae: 1.8862 - mse: 7.3754 - val_loss: 5.9742 - val_mae: 2.0858 - val_mse: 5.9742\n",
      "Epoch 250/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.3256 - mae: 1.8854 - mse: 7.3256 - val_loss: 5.9869 - val_mae: 2.1043 - val_mse: 5.9869\n",
      "Epoch 251/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.2873 - mae: 1.8783 - mse: 7.2873 - val_loss: 5.9350 - val_mae: 2.0782 - val_mse: 5.9350\n",
      "Epoch 252/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.2894 - mae: 1.8769 - mse: 7.2894 - val_loss: 6.0095 - val_mae: 2.0971 - val_mse: 6.0095\n",
      "Epoch 253/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.2755 - mae: 1.8712 - mse: 7.2755 - val_loss: 5.8777 - val_mae: 2.0694 - val_mse: 5.8777\n",
      "Epoch 254/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.2510 - mae: 1.8733 - mse: 7.2510 - val_loss: 5.8953 - val_mae: 2.0883 - val_mse: 5.8953\n",
      "Epoch 255/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.2209 - mae: 1.8581 - mse: 7.2209 - val_loss: 5.9608 - val_mae: 2.1081 - val_mse: 5.9608\n",
      "Epoch 256/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.2127 - mae: 1.8571 - mse: 7.2127 - val_loss: 5.8156 - val_mae: 2.0716 - val_mse: 5.8156\n",
      "Epoch 257/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.1898 - mae: 1.8589 - mse: 7.1898 - val_loss: 5.7585 - val_mae: 2.0535 - val_mse: 5.7585\n",
      "Epoch 258/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.1715 - mae: 1.8569 - mse: 7.1715 - val_loss: 5.7457 - val_mae: 2.0415 - val_mse: 5.7457\n",
      "Epoch 259/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.1804 - mae: 1.8557 - mse: 7.1804 - val_loss: 5.8529 - val_mae: 2.0765 - val_mse: 5.8529\n",
      "Epoch 260/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.1867 - mae: 1.8548 - mse: 7.1867 - val_loss: 5.7711 - val_mae: 2.0638 - val_mse: 5.7711\n",
      "Epoch 261/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.2152 - mae: 1.8538 - mse: 7.2152 - val_loss: 5.6925 - val_mae: 2.0293 - val_mse: 5.6925\n",
      "Epoch 262/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.1246 - mae: 1.8521 - mse: 7.1246 - val_loss: 5.7428 - val_mae: 2.0569 - val_mse: 5.7428\n",
      "Epoch 263/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.1003 - mae: 1.8512 - mse: 7.1003 - val_loss: 5.7609 - val_mae: 2.0666 - val_mse: 5.7609\n",
      "Epoch 264/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.1849 - mae: 1.8712 - mse: 7.1849 - val_loss: 6.0623 - val_mae: 2.1431 - val_mse: 6.0623\n",
      "Epoch 265/1000\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.2090 - mae: 1.8707 - mse: 7.2090 - val_loss: 5.7080 - val_mae: 2.0196 - val_mse: 5.7080\n",
      "Epoch 266/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0494 - mae: 1.8416 - mse: 7.0494 - val_loss: 5.8866 - val_mae: 2.1151 - val_mse: 5.8866\n",
      "Epoch 267/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.1138 - mae: 1.8471 - mse: 7.1138 - val_loss: 5.5804 - val_mae: 2.0281 - val_mse: 5.5804\n",
      "Epoch 268/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0596 - mae: 1.8373 - mse: 7.0596 - val_loss: 5.6112 - val_mae: 2.0307 - val_mse: 5.6112\n",
      "Epoch 269/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0658 - mae: 1.8392 - mse: 7.0658 - val_loss: 5.7916 - val_mae: 2.0754 - val_mse: 5.7916\n",
      "Epoch 270/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0206 - mae: 1.8334 - mse: 7.0206 - val_loss: 5.5747 - val_mae: 2.0071 - val_mse: 5.5747\n",
      "Epoch 271/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.9962 - mae: 1.8296 - mse: 6.9962 - val_loss: 5.6502 - val_mae: 2.0354 - val_mse: 5.6502\n",
      "Epoch 272/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0011 - mae: 1.8321 - mse: 7.0011 - val_loss: 5.7084 - val_mae: 2.0545 - val_mse: 5.7084\n",
      "Epoch 273/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0487 - mae: 1.8325 - mse: 7.0487 - val_loss: 5.6223 - val_mae: 2.0282 - val_mse: 5.6223\n",
      "Epoch 274/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 7.0006 - mae: 1.8299 - mse: 7.0006 - val_loss: 5.5432 - val_mae: 2.0018 - val_mse: 5.5432\n",
      "Epoch 275/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.9853 - mae: 1.8295 - mse: 6.9853 - val_loss: 5.6525 - val_mae: 2.0340 - val_mse: 5.6525\n",
      "Epoch 276/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.0010 - mae: 1.8301 - mse: 7.0010 - val_loss: 5.5340 - val_mae: 1.9976 - val_mse: 5.5340\n",
      "Epoch 277/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.9217 - mae: 1.8168 - mse: 6.9217 - val_loss: 5.6160 - val_mae: 2.0250 - val_mse: 5.6160\n",
      "Epoch 278/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.9101 - mae: 1.8188 - mse: 6.9101 - val_loss: 5.6699 - val_mae: 2.0315 - val_mse: 5.6699\n",
      "Epoch 279/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.9702 - mae: 1.8289 - mse: 6.9702 - val_loss: 5.4758 - val_mae: 1.9683 - val_mse: 5.4758\n",
      "Epoch 280/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.9210 - mae: 1.8181 - mse: 6.9210 - val_loss: 5.6890 - val_mae: 2.0415 - val_mse: 5.6890\n",
      "Epoch 281/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.8878 - mae: 1.8204 - mse: 6.8878 - val_loss: 5.7247 - val_mae: 2.0490 - val_mse: 5.7247\n",
      "Epoch 282/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.8670 - mae: 1.8220 - mse: 6.8670 - val_loss: 5.5743 - val_mae: 1.9978 - val_mse: 5.5743\n",
      "Epoch 283/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.8815 - mae: 1.8191 - mse: 6.8815 - val_loss: 5.7265 - val_mae: 2.0392 - val_mse: 5.7265\n",
      "Epoch 284/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.8687 - mae: 1.8216 - mse: 6.8687 - val_loss: 5.7906 - val_mae: 2.0612 - val_mse: 5.7906\n",
      "Epoch 285/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.8499 - mae: 1.8063 - mse: 6.8499 - val_loss: 5.6092 - val_mae: 2.0182 - val_mse: 5.6092\n",
      "Epoch 286/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.8130 - mae: 1.8090 - mse: 6.8130 - val_loss: 5.5307 - val_mae: 1.9926 - val_mse: 5.5307\n",
      "Epoch 287/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.8362 - mae: 1.8111 - mse: 6.8362 - val_loss: 5.5115 - val_mae: 1.9840 - val_mse: 5.5115\n",
      "Epoch 288/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7888 - mae: 1.8129 - mse: 6.7888 - val_loss: 5.5221 - val_mae: 1.9885 - val_mse: 5.5221\n",
      "Epoch 289/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7828 - mae: 1.8157 - mse: 6.7828 - val_loss: 5.7125 - val_mae: 2.0451 - val_mse: 5.7125\n",
      "Epoch 290/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7803 - mae: 1.8074 - mse: 6.7803 - val_loss: 5.5222 - val_mae: 2.0003 - val_mse: 5.5222\n",
      "Epoch 291/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7768 - mae: 1.8011 - mse: 6.7768 - val_loss: 5.6710 - val_mae: 2.0319 - val_mse: 5.6710\n",
      "Epoch 292/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7326 - mae: 1.7944 - mse: 6.7326 - val_loss: 5.5041 - val_mae: 1.9986 - val_mse: 5.5041\n",
      "Epoch 293/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 6.7159 - mae: 1.7972 - mse: 6.7159 - val_loss: 5.5595 - val_mae: 2.0054 - val_mse: 5.5595\n",
      "Epoch 294/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.6994 - mae: 1.7895 - mse: 6.6994 - val_loss: 5.4625 - val_mae: 1.9768 - val_mse: 5.4625\n",
      "Epoch 295/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.7098 - mae: 1.7897 - mse: 6.7098 - val_loss: 5.5552 - val_mae: 2.0019 - val_mse: 5.5552\n",
      "Epoch 296/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.6696 - mae: 1.7874 - mse: 6.6696 - val_loss: 5.5371 - val_mae: 2.0038 - val_mse: 5.5371\n",
      "Epoch 297/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.6902 - mae: 1.7891 - mse: 6.6902 - val_loss: 5.4630 - val_mae: 1.9825 - val_mse: 5.4630\n",
      "Epoch 298/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7154 - mae: 1.7871 - mse: 6.7154 - val_loss: 5.6378 - val_mae: 2.0290 - val_mse: 5.6378\n",
      "Epoch 299/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7017 - mae: 1.7991 - mse: 6.7017 - val_loss: 5.3394 - val_mae: 1.9381 - val_mse: 5.3394\n",
      "Epoch 300/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.6889 - mae: 1.7840 - mse: 6.6889 - val_loss: 5.6818 - val_mae: 2.0285 - val_mse: 5.6818\n",
      "Epoch 301/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.6123 - mae: 1.7764 - mse: 6.6123 - val_loss: 5.4001 - val_mae: 1.9605 - val_mse: 5.4001\n",
      "Epoch 302/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.6305 - mae: 1.7850 - mse: 6.6305 - val_loss: 5.3829 - val_mae: 1.9603 - val_mse: 5.3829\n",
      "Epoch 303/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.6243 - mae: 1.7855 - mse: 6.6243 - val_loss: 5.6975 - val_mae: 2.0327 - val_mse: 5.6975\n",
      "Epoch 304/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.6541 - mae: 1.7931 - mse: 6.6541 - val_loss: 5.3398 - val_mae: 1.9483 - val_mse: 5.3398\n",
      "Epoch 305/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5571 - mae: 1.7704 - mse: 6.5571 - val_loss: 5.6229 - val_mae: 2.0239 - val_mse: 5.6229\n",
      "Epoch 306/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5796 - mae: 1.7711 - mse: 6.5796 - val_loss: 5.5921 - val_mae: 2.0162 - val_mse: 5.5921\n",
      "Epoch 307/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5454 - mae: 1.7712 - mse: 6.5454 - val_loss: 5.3887 - val_mae: 1.9599 - val_mse: 5.3887\n",
      "Epoch 308/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5338 - mae: 1.7753 - mse: 6.5338 - val_loss: 5.5777 - val_mae: 1.9926 - val_mse: 5.5777\n",
      "Epoch 309/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.5902 - mae: 1.7740 - mse: 6.5902 - val_loss: 5.7974 - val_mae: 2.0489 - val_mse: 5.7974\n",
      "Epoch 310/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.4941 - mae: 1.7638 - mse: 6.4941 - val_loss: 5.4003 - val_mae: 1.9500 - val_mse: 5.4003\n",
      "Epoch 311/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5253 - mae: 1.7733 - mse: 6.5253 - val_loss: 5.3041 - val_mae: 1.9350 - val_mse: 5.3041\n",
      "Epoch 312/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5325 - mae: 1.7677 - mse: 6.5325 - val_loss: 5.6656 - val_mae: 2.0192 - val_mse: 5.6656\n",
      "Epoch 313/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5315 - mae: 1.7579 - mse: 6.5315 - val_loss: 5.4396 - val_mae: 1.9766 - val_mse: 5.4396\n",
      "Epoch 314/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.5044 - mae: 1.7527 - mse: 6.5044 - val_loss: 5.4753 - val_mae: 1.9846 - val_mse: 5.4753\n",
      "Epoch 315/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4766 - mae: 1.7662 - mse: 6.4766 - val_loss: 5.2709 - val_mae: 1.9223 - val_mse: 5.2709\n",
      "Epoch 316/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4746 - mae: 1.7641 - mse: 6.4746 - val_loss: 5.3806 - val_mae: 1.9595 - val_mse: 5.3806\n",
      "Epoch 317/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4599 - mae: 1.7589 - mse: 6.4599 - val_loss: 5.4483 - val_mae: 1.9694 - val_mse: 5.4483\n",
      "Epoch 318/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4131 - mae: 1.7526 - mse: 6.4131 - val_loss: 5.5156 - val_mae: 1.9835 - val_mse: 5.5156\n",
      "Epoch 319/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.4101 - mae: 1.7469 - mse: 6.4101 - val_loss: 5.5551 - val_mae: 1.9976 - val_mse: 5.5551\n",
      "Epoch 320/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4259 - mae: 1.7478 - mse: 6.4259 - val_loss: 5.5575 - val_mae: 1.9948 - val_mse: 5.5575\n",
      "Epoch 321/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.3810 - mae: 1.7468 - mse: 6.3810 - val_loss: 5.3351 - val_mae: 1.9342 - val_mse: 5.3351\n",
      "Epoch 322/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.4373 - mae: 1.7655 - mse: 6.4373 - val_loss: 5.3696 - val_mae: 1.9386 - val_mse: 5.3696\n",
      "Epoch 323/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.3833 - mae: 1.7517 - mse: 6.3833 - val_loss: 5.6437 - val_mae: 2.0061 - val_mse: 5.6437\n",
      "Epoch 324/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.3566 - mae: 1.7400 - mse: 6.3566 - val_loss: 5.3894 - val_mae: 1.9560 - val_mse: 5.3894\n",
      "Epoch 325/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.4198 - mae: 1.7522 - mse: 6.4198 - val_loss: 5.2201 - val_mae: 1.9084 - val_mse: 5.2201\n",
      "Epoch 326/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.3768 - mae: 1.7542 - mse: 6.3768 - val_loss: 5.8006 - val_mae: 2.0337 - val_mse: 5.8006\n",
      "Epoch 327/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.3438 - mae: 1.7348 - mse: 6.3438 - val_loss: 5.4607 - val_mae: 1.9653 - val_mse: 5.4607\n",
      "Epoch 328/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.3392 - mae: 1.7361 - mse: 6.3392 - val_loss: 5.2969 - val_mae: 1.9286 - val_mse: 5.2969\n",
      "Epoch 329/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.3025 - mae: 1.7308 - mse: 6.3025 - val_loss: 5.5752 - val_mae: 1.9860 - val_mse: 5.5752\n",
      "Epoch 330/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2953 - mae: 1.7282 - mse: 6.2953 - val_loss: 5.3468 - val_mae: 1.9363 - val_mse: 5.3468\n",
      "Epoch 331/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.2708 - mae: 1.7267 - mse: 6.2708 - val_loss: 5.4039 - val_mae: 1.9509 - val_mse: 5.4039\n",
      "Epoch 332/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2815 - mae: 1.7315 - mse: 6.2815 - val_loss: 5.4620 - val_mae: 1.9626 - val_mse: 5.4620\n",
      "Epoch 333/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2916 - mae: 1.7246 - mse: 6.2916 - val_loss: 5.4212 - val_mae: 1.9645 - val_mse: 5.4212\n",
      "Epoch 334/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2945 - mae: 1.7321 - mse: 6.2945 - val_loss: 5.1914 - val_mae: 1.8916 - val_mse: 5.1914\n",
      "Epoch 335/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2596 - mae: 1.7173 - mse: 6.2596 - val_loss: 5.4452 - val_mae: 1.9610 - val_mse: 5.4452\n",
      "Epoch 336/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2431 - mae: 1.7181 - mse: 6.2431 - val_loss: 5.2721 - val_mae: 1.9229 - val_mse: 5.2721\n",
      "Epoch 337/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2406 - mae: 1.7277 - mse: 6.2406 - val_loss: 5.4348 - val_mae: 1.9528 - val_mse: 5.4348\n",
      "Epoch 338/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2133 - mae: 1.7134 - mse: 6.2133 - val_loss: 5.4769 - val_mae: 1.9697 - val_mse: 5.4769\n",
      "Epoch 339/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2085 - mae: 1.7165 - mse: 6.2085 - val_loss: 5.4812 - val_mae: 1.9596 - val_mse: 5.4812\n",
      "Epoch 340/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1937 - mae: 1.7188 - mse: 6.1937 - val_loss: 5.3828 - val_mae: 1.9429 - val_mse: 5.3828\n",
      "Epoch 341/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.1926 - mae: 1.7105 - mse: 6.1926 - val_loss: 5.5370 - val_mae: 1.9732 - val_mse: 5.5370\n",
      "Epoch 342/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1939 - mae: 1.7181 - mse: 6.1939 - val_loss: 5.4084 - val_mae: 1.9362 - val_mse: 5.4084\n",
      "Epoch 343/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.3837 - mae: 1.7612 - mse: 6.3837 - val_loss: 5.1521 - val_mae: 1.8644 - val_mse: 5.1521\n",
      "Epoch 344/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.1802 - mae: 1.7214 - mse: 6.1802 - val_loss: 5.9045 - val_mae: 2.0357 - val_mse: 5.9045\n",
      "Epoch 345/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.2142 - mae: 1.7207 - mse: 6.2142 - val_loss: 5.3463 - val_mae: 1.9293 - val_mse: 5.3463\n",
      "Epoch 346/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1366 - mae: 1.7111 - mse: 6.1366 - val_loss: 5.3974 - val_mae: 1.9453 - val_mse: 5.3974\n",
      "Epoch 347/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1058 - mae: 1.7017 - mse: 6.1058 - val_loss: 5.4426 - val_mae: 1.9516 - val_mse: 5.4426\n",
      "Epoch 348/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1166 - mae: 1.7090 - mse: 6.1166 - val_loss: 5.3844 - val_mae: 1.9346 - val_mse: 5.3844\n",
      "Epoch 349/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.1090 - mae: 1.7046 - mse: 6.1090 - val_loss: 5.3046 - val_mae: 1.9266 - val_mse: 5.3046\n",
      "Epoch 350/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.0955 - mae: 1.7108 - mse: 6.0955 - val_loss: 5.4712 - val_mae: 1.9524 - val_mse: 5.4712\n",
      "Epoch 351/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0577 - mae: 1.6992 - mse: 6.0577 - val_loss: 5.2437 - val_mae: 1.9136 - val_mse: 5.2437\n",
      "Epoch 352/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0869 - mae: 1.6984 - mse: 6.0869 - val_loss: 5.2896 - val_mae: 1.9193 - val_mse: 5.2896\n",
      "Epoch 353/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0717 - mae: 1.6939 - mse: 6.0717 - val_loss: 5.3112 - val_mae: 1.9141 - val_mse: 5.3112\n",
      "Epoch 354/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0442 - mae: 1.6897 - mse: 6.0442 - val_loss: 5.4076 - val_mae: 1.9330 - val_mse: 5.4076\n",
      "Epoch 355/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0582 - mae: 1.6886 - mse: 6.0582 - val_loss: 5.3083 - val_mae: 1.9177 - val_mse: 5.3083\n",
      "Epoch 356/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0580 - mae: 1.7017 - mse: 6.0580 - val_loss: 5.3161 - val_mae: 1.9097 - val_mse: 5.3161\n",
      "Epoch 357/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0470 - mae: 1.6957 - mse: 6.0470 - val_loss: 5.1272 - val_mae: 1.8808 - val_mse: 5.1272\n",
      "Epoch 358/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0092 - mae: 1.6981 - mse: 6.0092 - val_loss: 5.3450 - val_mae: 1.9194 - val_mse: 5.3450\n",
      "Epoch 359/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0316 - mae: 1.6923 - mse: 6.0316 - val_loss: 5.2086 - val_mae: 1.8967 - val_mse: 5.2086\n",
      "Epoch 360/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.0194 - mae: 1.6914 - mse: 6.0194 - val_loss: 4.9722 - val_mae: 1.8594 - val_mse: 4.9722\n",
      "Epoch 361/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0080 - mae: 1.6824 - mse: 6.0080 - val_loss: 5.2541 - val_mae: 1.9027 - val_mse: 5.2541\n",
      "Epoch 362/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.0295 - mae: 1.7056 - mse: 6.0295 - val_loss: 5.1352 - val_mae: 1.8732 - val_mse: 5.1352\n",
      "Epoch 363/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 6.0716 - mae: 1.6977 - mse: 6.0716 - val_loss: 5.0351 - val_mae: 1.8655 - val_mse: 5.0351\n",
      "Epoch 364/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.9542 - mae: 1.6873 - mse: 5.9542 - val_loss: 5.6526 - val_mae: 1.9787 - val_mse: 5.6526\n",
      "Epoch 365/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.9869 - mae: 1.6877 - mse: 5.9869 - val_loss: 5.2323 - val_mae: 1.9008 - val_mse: 5.2323\n",
      "Epoch 366/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.9413 - mae: 1.6852 - mse: 5.9413 - val_loss: 5.0180 - val_mae: 1.8426 - val_mse: 5.0180\n",
      "Epoch 367/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.9162 - mae: 1.6842 - mse: 5.9162 - val_loss: 5.3310 - val_mae: 1.9201 - val_mse: 5.3310\n",
      "Epoch 368/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.9386 - mae: 1.6796 - mse: 5.9386 - val_loss: 5.3339 - val_mae: 1.9321 - val_mse: 5.3339\n",
      "Epoch 369/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.9269 - mae: 1.6776 - mse: 5.9269 - val_loss: 5.2626 - val_mae: 1.9102 - val_mse: 5.2626\n",
      "Epoch 370/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8899 - mae: 1.6850 - mse: 5.8899 - val_loss: 5.3698 - val_mae: 1.9248 - val_mse: 5.3698\n",
      "Epoch 371/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8998 - mae: 1.6716 - mse: 5.8998 - val_loss: 5.1845 - val_mae: 1.8924 - val_mse: 5.1845\n",
      "Epoch 372/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8898 - mae: 1.6691 - mse: 5.8898 - val_loss: 5.1526 - val_mae: 1.8794 - val_mse: 5.1526\n",
      "Epoch 373/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8552 - mae: 1.6693 - mse: 5.8552 - val_loss: 5.1921 - val_mae: 1.8953 - val_mse: 5.1921\n",
      "Epoch 374/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8880 - mae: 1.6758 - mse: 5.8880 - val_loss: 5.3640 - val_mae: 1.9253 - val_mse: 5.3640\n",
      "Epoch 375/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.9056 - mae: 1.6728 - mse: 5.9056 - val_loss: 5.1317 - val_mae: 1.8992 - val_mse: 5.1317\n",
      "Epoch 376/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8339 - mae: 1.6770 - mse: 5.8339 - val_loss: 5.3120 - val_mae: 1.9071 - val_mse: 5.3120\n",
      "Epoch 377/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8322 - mae: 1.6669 - mse: 5.8322 - val_loss: 5.1600 - val_mae: 1.8938 - val_mse: 5.1600\n",
      "Epoch 378/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8107 - mae: 1.6619 - mse: 5.8107 - val_loss: 5.2732 - val_mae: 1.9124 - val_mse: 5.2732\n",
      "Epoch 379/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8506 - mae: 1.6652 - mse: 5.8506 - val_loss: 5.3365 - val_mae: 1.9272 - val_mse: 5.3365\n",
      "Epoch 380/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8816 - mae: 1.6683 - mse: 5.8816 - val_loss: 5.3197 - val_mae: 1.9190 - val_mse: 5.3197\n",
      "Epoch 381/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8289 - mae: 1.6670 - mse: 5.8289 - val_loss: 5.0681 - val_mae: 1.8544 - val_mse: 5.0681\n",
      "Epoch 382/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.8418 - mae: 1.6680 - mse: 5.8418 - val_loss: 5.1469 - val_mae: 1.8859 - val_mse: 5.1469\n",
      "Epoch 383/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7731 - mae: 1.6571 - mse: 5.7731 - val_loss: 5.2061 - val_mae: 1.8945 - val_mse: 5.2061\n",
      "Epoch 384/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7646 - mae: 1.6581 - mse: 5.7646 - val_loss: 5.1627 - val_mae: 1.8837 - val_mse: 5.1627\n",
      "Epoch 385/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7811 - mae: 1.6611 - mse: 5.7811 - val_loss: 5.2090 - val_mae: 1.9050 - val_mse: 5.2090\n",
      "Epoch 386/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7780 - mae: 1.6587 - mse: 5.7780 - val_loss: 5.3681 - val_mae: 1.9341 - val_mse: 5.3681\n",
      "Epoch 387/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7642 - mae: 1.6620 - mse: 5.7642 - val_loss: 5.0394 - val_mae: 1.8655 - val_mse: 5.0394\n",
      "Epoch 388/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7706 - mae: 1.6540 - mse: 5.7706 - val_loss: 5.3287 - val_mae: 1.9101 - val_mse: 5.3287\n",
      "Epoch 389/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7318 - mae: 1.6526 - mse: 5.7318 - val_loss: 5.0905 - val_mae: 1.8791 - val_mse: 5.0905\n",
      "Epoch 390/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.7378 - mae: 1.6496 - mse: 5.7378 - val_loss: 5.1478 - val_mae: 1.8837 - val_mse: 5.1478\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7165 - mae: 1.6511 - mse: 5.7165 - val_loss: 5.1327 - val_mae: 1.8774 - val_mse: 5.1327\n",
      "Epoch 392/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7166 - mae: 1.6482 - mse: 5.7166 - val_loss: 5.1599 - val_mae: 1.8921 - val_mse: 5.1599\n",
      "Epoch 393/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7292 - mae: 1.6590 - mse: 5.7292 - val_loss: 5.0901 - val_mae: 1.8750 - val_mse: 5.0901\n",
      "Epoch 394/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7141 - mae: 1.6572 - mse: 5.7141 - val_loss: 5.1491 - val_mae: 1.9034 - val_mse: 5.1491\n",
      "Epoch 395/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7539 - mae: 1.6524 - mse: 5.7539 - val_loss: 5.3666 - val_mae: 1.9219 - val_mse: 5.3666\n",
      "Epoch 396/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6509 - mae: 1.6360 - mse: 5.6509 - val_loss: 5.0417 - val_mae: 1.8696 - val_mse: 5.0417\n",
      "Epoch 397/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7082 - mae: 1.6552 - mse: 5.7082 - val_loss: 4.9170 - val_mae: 1.8488 - val_mse: 4.9170\n",
      "Epoch 398/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6657 - mae: 1.6474 - mse: 5.6657 - val_loss: 5.3897 - val_mae: 1.9209 - val_mse: 5.3897\n",
      "Epoch 399/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6783 - mae: 1.6411 - mse: 5.6783 - val_loss: 5.1928 - val_mae: 1.9122 - val_mse: 5.1928\n",
      "Epoch 400/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6506 - mae: 1.6452 - mse: 5.6506 - val_loss: 4.9979 - val_mae: 1.8361 - val_mse: 4.9979\n",
      "Epoch 401/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6701 - mae: 1.6532 - mse: 5.6701 - val_loss: 5.2302 - val_mae: 1.9113 - val_mse: 5.2302\n",
      "Epoch 402/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.6649 - mae: 1.6434 - mse: 5.6649 - val_loss: 5.0502 - val_mae: 1.8625 - val_mse: 5.0502\n",
      "Epoch 403/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.5960 - mae: 1.6386 - mse: 5.5960 - val_loss: 5.3151 - val_mae: 1.9090 - val_mse: 5.3151\n",
      "Epoch 404/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.6182 - mae: 1.6350 - mse: 5.6182 - val_loss: 5.1777 - val_mae: 1.8938 - val_mse: 5.1777\n",
      "Epoch 405/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6478 - mae: 1.6423 - mse: 5.6478 - val_loss: 5.1388 - val_mae: 1.8776 - val_mse: 5.1388\n",
      "Epoch 406/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.6491 - mae: 1.6493 - mse: 5.6491 - val_loss: 5.0449 - val_mae: 1.8837 - val_mse: 5.0449\n",
      "Epoch 407/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5803 - mae: 1.6351 - mse: 5.5803 - val_loss: 5.2538 - val_mae: 1.9001 - val_mse: 5.2538\n",
      "Epoch 408/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.6103 - mae: 1.6436 - mse: 5.6103 - val_loss: 5.3186 - val_mae: 1.9119 - val_mse: 5.3186\n",
      "Epoch 409/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7286 - mae: 1.6570 - mse: 5.7286 - val_loss: 5.1329 - val_mae: 1.9074 - val_mse: 5.1329\n",
      "Epoch 410/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5804 - mae: 1.6321 - mse: 5.5804 - val_loss: 5.0391 - val_mae: 1.8392 - val_mse: 5.0391\n",
      "Epoch 411/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6772 - mae: 1.6586 - mse: 5.6772 - val_loss: 5.5577 - val_mae: 1.9635 - val_mse: 5.5577\n",
      "Epoch 412/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.7795 - mae: 1.6712 - mse: 5.7795 - val_loss: 4.9849 - val_mae: 1.8765 - val_mse: 4.9849\n",
      "Epoch 413/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5405 - mae: 1.6278 - mse: 5.5405 - val_loss: 5.2574 - val_mae: 1.8958 - val_mse: 5.2574\n",
      "Epoch 414/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5719 - mae: 1.6254 - mse: 5.5719 - val_loss: 5.2104 - val_mae: 1.9101 - val_mse: 5.2104\n",
      "Epoch 415/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5587 - mae: 1.6250 - mse: 5.5587 - val_loss: 5.2701 - val_mae: 1.9043 - val_mse: 5.2701\n",
      "Epoch 416/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.6432 - mae: 1.6392 - mse: 5.6432 - val_loss: 4.9170 - val_mae: 1.8514 - val_mse: 4.9170\n",
      "Epoch 417/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5860 - mae: 1.6553 - mse: 5.5860 - val_loss: 5.2230 - val_mae: 1.8745 - val_mse: 5.2230\n",
      "Epoch 418/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4979 - mae: 1.6203 - mse: 5.4979 - val_loss: 5.2712 - val_mae: 1.9222 - val_mse: 5.2712\n",
      "Epoch 419/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.5172 - mae: 1.6141 - mse: 5.5172 - val_loss: 5.3249 - val_mae: 1.9395 - val_mse: 5.3249\n",
      "Epoch 420/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.5338 - mae: 1.6214 - mse: 5.5338 - val_loss: 5.2152 - val_mae: 1.8792 - val_mse: 5.2152\n",
      "Epoch 421/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.5339 - mae: 1.6186 - mse: 5.5339 - val_loss: 5.0442 - val_mae: 1.8716 - val_mse: 5.0442\n",
      "Epoch 422/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.5085 - mae: 1.6170 - mse: 5.5085 - val_loss: 5.3950 - val_mae: 1.9256 - val_mse: 5.3950\n",
      "Epoch 423/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.5097 - mae: 1.6113 - mse: 5.5097 - val_loss: 5.0781 - val_mae: 1.8743 - val_mse: 5.0781\n",
      "Epoch 424/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4570 - mae: 1.6155 - mse: 5.4570 - val_loss: 5.1075 - val_mae: 1.8562 - val_mse: 5.1075\n",
      "Epoch 425/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4577 - mae: 1.6088 - mse: 5.4577 - val_loss: 5.2326 - val_mae: 1.8924 - val_mse: 5.2326\n",
      "Epoch 426/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4547 - mae: 1.6124 - mse: 5.4547 - val_loss: 5.1432 - val_mae: 1.8680 - val_mse: 5.1432\n",
      "Epoch 427/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4458 - mae: 1.6152 - mse: 5.4458 - val_loss: 4.9138 - val_mae: 1.8485 - val_mse: 4.9138\n",
      "Epoch 428/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4859 - mae: 1.6348 - mse: 5.4859 - val_loss: 5.0777 - val_mae: 1.8511 - val_mse: 5.0777\n",
      "Epoch 429/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4591 - mae: 1.6231 - mse: 5.4591 - val_loss: 5.2958 - val_mae: 1.9083 - val_mse: 5.2958\n",
      "Epoch 430/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4559 - mae: 1.6210 - mse: 5.4559 - val_loss: 5.1446 - val_mae: 1.8633 - val_mse: 5.1446\n",
      "Epoch 431/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4116 - mae: 1.6110 - mse: 5.4116 - val_loss: 5.2008 - val_mae: 1.8882 - val_mse: 5.2008\n",
      "Epoch 432/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4489 - mae: 1.6159 - mse: 5.4489 - val_loss: 5.1172 - val_mae: 1.8810 - val_mse: 5.1172\n",
      "Epoch 433/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.4077 - mae: 1.6033 - mse: 5.4077 - val_loss: 5.1150 - val_mae: 1.8748 - val_mse: 5.1150\n",
      "Epoch 434/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4520 - mae: 1.6068 - mse: 5.4520 - val_loss: 5.2384 - val_mae: 1.8867 - val_mse: 5.2384\n",
      "Epoch 435/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.3908 - mae: 1.6063 - mse: 5.3908 - val_loss: 4.9178 - val_mae: 1.8324 - val_mse: 4.9178\n",
      "Epoch 436/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4369 - mae: 1.6248 - mse: 5.4369 - val_loss: 5.2534 - val_mae: 1.8916 - val_mse: 5.2534\n",
      "Epoch 437/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.3960 - mae: 1.6077 - mse: 5.3960 - val_loss: 5.3669 - val_mae: 1.9391 - val_mse: 5.3669\n",
      "Epoch 438/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3950 - mae: 1.5956 - mse: 5.3950 - val_loss: 5.1813 - val_mae: 1.8898 - val_mse: 5.1813\n",
      "Epoch 439/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3708 - mae: 1.6043 - mse: 5.3708 - val_loss: 5.0985 - val_mae: 1.8548 - val_mse: 5.0985\n",
      "Epoch 440/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3653 - mae: 1.6061 - mse: 5.3653 - val_loss: 5.1204 - val_mae: 1.8762 - val_mse: 5.1204\n",
      "Epoch 441/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3969 - mae: 1.6090 - mse: 5.3969 - val_loss: 5.3440 - val_mae: 1.9258 - val_mse: 5.3440\n",
      "Epoch 442/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4653 - mae: 1.6180 - mse: 5.4653 - val_loss: 4.9878 - val_mae: 1.8476 - val_mse: 4.9878\n",
      "Epoch 443/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.4250 - mae: 1.6265 - mse: 5.4250 - val_loss: 5.4484 - val_mae: 1.9037 - val_mse: 5.4484\n",
      "Epoch 444/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3837 - mae: 1.5961 - mse: 5.3837 - val_loss: 5.2431 - val_mae: 1.9181 - val_mse: 5.2431\n",
      "Epoch 445/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3738 - mae: 1.6056 - mse: 5.3738 - val_loss: 5.1005 - val_mae: 1.8456 - val_mse: 5.1005\n",
      "Epoch 446/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3460 - mae: 1.6012 - mse: 5.3460 - val_loss: 5.1657 - val_mae: 1.8844 - val_mse: 5.1657\n",
      "Epoch 447/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3573 - mae: 1.6114 - mse: 5.3573 - val_loss: 5.3464 - val_mae: 1.8955 - val_mse: 5.3464\n",
      "Epoch 448/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3458 - mae: 1.5977 - mse: 5.3458 - val_loss: 5.0340 - val_mae: 1.8651 - val_mse: 5.0340\n",
      "Epoch 449/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3466 - mae: 1.6021 - mse: 5.3466 - val_loss: 4.9749 - val_mae: 1.8317 - val_mse: 4.9749\n",
      "Epoch 450/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3854 - mae: 1.6065 - mse: 5.3854 - val_loss: 5.5172 - val_mae: 1.9515 - val_mse: 5.5172\n",
      "Epoch 451/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.3027 - mae: 1.5876 - mse: 5.3027 - val_loss: 5.1365 - val_mae: 1.8807 - val_mse: 5.1365\n",
      "Epoch 452/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.2769 - mae: 1.5916 - mse: 5.2769 - val_loss: 5.0512 - val_mae: 1.8555 - val_mse: 5.0512\n",
      "Epoch 453/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2910 - mae: 1.6000 - mse: 5.2910 - val_loss: 5.2333 - val_mae: 1.8864 - val_mse: 5.2333\n",
      "Epoch 454/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2629 - mae: 1.5834 - mse: 5.2629 - val_loss: 5.1498 - val_mae: 1.8713 - val_mse: 5.1498\n",
      "Epoch 455/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2826 - mae: 1.5806 - mse: 5.2826 - val_loss: 5.2325 - val_mae: 1.8806 - val_mse: 5.2325\n",
      "Epoch 456/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2548 - mae: 1.5884 - mse: 5.2548 - val_loss: 5.1874 - val_mae: 1.8660 - val_mse: 5.1874\n",
      "Epoch 457/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2576 - mae: 1.5924 - mse: 5.2576 - val_loss: 5.2435 - val_mae: 1.8950 - val_mse: 5.2435\n",
      "Epoch 458/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2942 - mae: 1.5914 - mse: 5.2942 - val_loss: 5.4272 - val_mae: 1.9334 - val_mse: 5.4272\n",
      "Epoch 459/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.2821 - mae: 1.6029 - mse: 5.2821 - val_loss: 5.1771 - val_mae: 1.8557 - val_mse: 5.1771\n",
      "Epoch 460/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2752 - mae: 1.5858 - mse: 5.2752 - val_loss: 5.0715 - val_mae: 1.8621 - val_mse: 5.0715\n",
      "Epoch 461/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2503 - mae: 1.5896 - mse: 5.2503 - val_loss: 5.4132 - val_mae: 1.9161 - val_mse: 5.4132\n",
      "Epoch 462/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2199 - mae: 1.5801 - mse: 5.2199 - val_loss: 5.1302 - val_mae: 1.8662 - val_mse: 5.1302\n",
      "Epoch 463/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2101 - mae: 1.5782 - mse: 5.2101 - val_loss: 5.3025 - val_mae: 1.8823 - val_mse: 5.3025\n",
      "Epoch 464/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.2195 - mae: 1.5773 - mse: 5.2195 - val_loss: 5.1376 - val_mae: 1.8619 - val_mse: 5.1376\n",
      "Epoch 465/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1973 - mae: 1.5769 - mse: 5.1973 - val_loss: 5.1971 - val_mae: 1.8611 - val_mse: 5.1971\n",
      "Epoch 466/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1965 - mae: 1.5762 - mse: 5.1965 - val_loss: 5.1963 - val_mae: 1.8856 - val_mse: 5.1963\n",
      "Epoch 467/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.2201 - mae: 1.5688 - mse: 5.2201 - val_loss: 5.4124 - val_mae: 1.9273 - val_mse: 5.4124\n",
      "Epoch 468/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1943 - mae: 1.5742 - mse: 5.1943 - val_loss: 5.0996 - val_mae: 1.8376 - val_mse: 5.0996\n",
      "Epoch 469/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1802 - mae: 1.5794 - mse: 5.1802 - val_loss: 5.0899 - val_mae: 1.8594 - val_mse: 5.0899\n",
      "Epoch 470/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1914 - mae: 1.5716 - mse: 5.1914 - val_loss: 5.1656 - val_mae: 1.8681 - val_mse: 5.1656\n",
      "Epoch 471/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1645 - mae: 1.5703 - mse: 5.1645 - val_loss: 5.2319 - val_mae: 1.8791 - val_mse: 5.2319\n",
      "Epoch 472/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1998 - mae: 1.5813 - mse: 5.1998 - val_loss: 5.4007 - val_mae: 1.9055 - val_mse: 5.4007\n",
      "Epoch 473/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1933 - mae: 1.5758 - mse: 5.1933 - val_loss: 5.1171 - val_mae: 1.8579 - val_mse: 5.1171\n",
      "Epoch 474/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5.1560 - mae: 1.5749 - mse: 5.1560 - val_loss: 5.2110 - val_mae: 1.8615 - val_mse: 5.2110\n",
      "Epoch 475/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.1504 - mae: 1.5662 - mse: 5.1504 - val_loss: 5.1684 - val_mae: 1.8637 - val_mse: 5.1684\n",
      "Epoch 476/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.1879 - mae: 1.5746 - mse: 5.1879 - val_loss: 5.2215 - val_mae: 1.8844 - val_mse: 5.2215\n",
      "Epoch 477/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.1466 - mae: 1.5696 - mse: 5.1466 - val_loss: 5.0371 - val_mae: 1.8400 - val_mse: 5.0371\n"
     ]
    }
   ],
   "source": [
    "#Now, lets train the model\n",
    "model = nn_model()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(train_features, train_labels, epochs=1000, verbose=1, validation_split = 0.1,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Root Mean Square Error on validation set: 2.244\n"
     ]
    }
   ],
   "source": [
    "# show RMSE measure to compare to Kaggle leaderboard on https://www.kaggle.com/c/boston-housing/leaderboard\n",
    "rmse_final = np.sqrt(float(hist['val_mse'].tail(1)))\n",
    "print()\n",
    "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           loss        mae         mse    val_loss    val_mae     val_mse  \\\n",
      "0    571.179443  22.005508  571.179443  470.933411  20.735037  470.933411   \n",
      "1    544.794922  21.465631  544.794922  447.052887  20.192919  447.052887   \n",
      "2    519.264954  20.927948  519.264954  422.171509  19.611822  422.171509   \n",
      "3    490.569000  20.309143  490.569000  396.087189  18.971205  396.087189   \n",
      "4    461.038361  19.645565  461.038361  366.755524  18.231638  366.755524   \n",
      "..          ...        ...         ...         ...        ...         ...   \n",
      "472    5.193298   1.575820    5.193298    5.117089   1.857873    5.117089   \n",
      "473    5.155988   1.574921    5.155988    5.211012   1.861485    5.211012   \n",
      "474    5.150421   1.566247    5.150421    5.168377   1.863712    5.168377   \n",
      "475    5.187897   1.574596    5.187897    5.221528   1.884422    5.221528   \n",
      "476    5.146584   1.569564    5.146584    5.037115   1.840041    5.037115   \n",
      "\n",
      "     epoch  \n",
      "0        0  \n",
      "1        1  \n",
      "2        2  \n",
      "3        3  \n",
      "4        4  \n",
      "..     ...  \n",
      "472    472  \n",
      "473    473  \n",
      "474    474  \n",
      "475    475  \n",
      "476    476  \n",
      "\n",
      "[477 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 15.4254 - mae: 2.6651 - mse: 15.4254\n",
      "Root Mean Square Error on test set: 3.928\n"
     ]
    }
   ],
   "source": [
    "mse, _, _ = model.evaluate(test_features, test_labels)\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, lets generate more test samples from test data and monitor them, as priduction data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (404,)\n",
      "(102, 13) (102,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_features), np.shape(train_labels))\n",
    "print(np.shape(test_features), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=np.shape(test_labels)[0]\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the ml_monotor library\n",
    "import ml_monitor\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-24 22:12:59,000 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n"
     ]
    }
   ],
   "source": [
    "my_monitor = ml_monitor.Monitor()\n",
    "my_monitor.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-24 22:16:04,059 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:09,061 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:14,062 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:19,064 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:24,065 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:29,067 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:34,068 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:39,069 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:44,071 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:49,071 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:54,073 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:16:59,074 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:04,074 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:09,075 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:14,077 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:19,078 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:24,080 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:29,082 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:34,083 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:39,084 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:44,086 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:49,087 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:54,090 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:17:59,090 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:04,093 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:09,095 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:14,096 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:19,099 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:24,100 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:29,101 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:34,103 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:39,103 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:44,105 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:49,108 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:54,110 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:18:59,112 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:04,115 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:09,118 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:14,120 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:19,123 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:24,125 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:29,128 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:34,130 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:39,133 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:44,136 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:49,138 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:54,139 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:19:59,142 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:04,143 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:09,144 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:14,145 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:19,146 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:24,147 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:29,148 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:34,149 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:39,150 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:44,152 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:49,153 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n",
      "2021-05-24 22:20:54,154 [71814] INFO     ml_monitor.logging: Starting metrics logging thread...\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    indx=random.randint(0,num-1)\n",
    "    x_batch=np.expand_dims(test_features[indx,:],axis=0)\n",
    "    y_batch=np.expand_dims(test_labels[indx],axis=0)\n",
    "    mse, _, _ = model.evaluate(x_batch, y_batch, verbose=0)\n",
    "    my_monitor.monitor(\"n_rmse\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
